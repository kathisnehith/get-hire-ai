{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d4b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/hiring_manager_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'hiring_manager_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/email_guide.pdf\n",
      "Number of chunks:---- 2\n",
      " metadata:---- 2 [{'section': 'email_guide'}, {'section': 'email_guide'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/recruiter_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'recruiter_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/senioremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'senioremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/peeremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'peeremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/base_email_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'base_email_template'}]\n",
      "Total chunks: 7\n",
      "First chunk metadata: {'section': 'hiring_manager_template'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import langchain\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "pdf_folder_path = r\"/Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide\"\n",
    "chunk_size_len = 2000\n",
    "chunk_overlap_len = 50\n",
    "email_text_chunks = []\n",
    "email_metadata = []\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        rag_file_path =os.path.join(pdf_folder_path, filename)\n",
    "        print(f\"Processing file: {rag_file_path}\")\n",
    "        loader = PyPDFLoader(file_path=rag_file_path)\n",
    "        document = loader.load()\n",
    "        text_splitter_char = CharacterTextSplitter(chunk_size=chunk_size_len, chunk_overlap=chunk_overlap_len, separator=\"\\n\")\n",
    "        split_documents = text_splitter_char.split_documents(document)\n",
    "        print(f\"Number of chunks:---- {len(split_documents)}\") \n",
    "        doc_tag = filename.split('.')[0]\n",
    "        metadatas = [{\"section\": doc_tag} for _ in split_documents]     \n",
    "        print(f\" metadata:---- {len(metadatas)} {metadatas}\")\n",
    "        email_text_chunks.extend(split_documents)\n",
    "        email_metadata.extend(metadatas) \n",
    "\n",
    "print(f\"Total chunks: {len(email_text_chunks)}\")\n",
    "print(f\"First chunk metadata: {email_metadata[0]}\")\n",
    "\n",
    "### tokenization function counting of texts\n",
    "def num_tokens_from_string(input_text: str, encoding_name: str) -> str:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(input_text))\n",
    "    return num_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef718456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 metadata: {'section': 'hiring_manager_template'}\n",
      "Embedding Vector (first 5 dims): [0.000710678577888757, -0.006397498305886984, -0.004142611753195524, -0.035692475736141205, -0.015235316939651966]\n",
      "Chunk 1 metadata: {'section': 'email_guide'}\n",
      "Embedding Vector (first 5 dims): [-0.017011938616633415, -0.031198788434267044, -0.007742120418697596, -0.04854850843548775, -0.012551678344607353]\n",
      "Chunk 2 metadata: {'section': 'email_guide'}\n",
      "Embedding Vector (first 5 dims): [-0.007910573855042458, -0.040177181363105774, -0.0038782358169555664, -0.01838161237537861, -0.01209108717739582]\n",
      "Chunk 3 metadata: {'section': 'recruiter_template'}\n",
      "Embedding Vector (first 5 dims): [-0.019944479689002037, -0.03546753153204918, -0.012463296763598919, -0.02058526501059532, -0.03879962116479874]\n",
      "Chunk 4 metadata: {'section': 'senioremployee_template'}\n",
      "Embedding Vector (first 5 dims): [-0.018149359151721, 0.008191158063709736, -0.01531713455915451, -0.015003359876573086, -0.015919910743832588]\n",
      "Chunk 5 metadata: {'section': 'peeremployee_template'}\n",
      "Embedding Vector (first 5 dims): [-0.03984778746962547, 0.004485694691538811, -0.018184378743171692, -0.02813827246427536, -0.0160583034157753]\n",
      "Chunk 6 metadata: {'section': 'base_email_template'}\n",
      "Embedding Vector (first 5 dims): [0.0030544132459908724, -0.026033055037260056, -0.006182117387652397, -0.032746534794569016, -0.017941700294613838]\n"
     ]
    }
   ],
   "source": [
    "## embedding function using OPENAI-EMBEDDING-3-LARGE\n",
    "\n",
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=[chunk.page_content for chunk in email_text_chunks],\n",
    "    model=embedding_model,\n",
    ")\n",
    "#print(response)\n",
    "embeddings = [item.embedding for item in response.data]\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Display results\n",
    "for i, item in enumerate(response.data):\n",
    "    print(f\"Chunk {i} metadata: {email_metadata[i]}\")\n",
    "    print(f\"Embedding Vector (first 5 dims): {item.embedding[:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29579bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"./chromadb_email_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"git_email_rag\")\n",
    "collection.add(\n",
    "    documents=[chunk.page_content for chunk in email_text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    metadatas=email_metadata,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(email_text_chunks))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de451e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chroma collection populated with 7 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "doc_count = collection.count()\n",
    "print(f\"âœ… Chroma collection populated with {doc_count} documents\")\n",
    "\n",
    "# ðŸ”’ Prevent accidental zipping of empty DB\n",
    "if doc_count == 0:\n",
    "    raise Exception(\"âŒ ERROR: No documents added to ChromaDB! Skipping zip to avoid saving empty DB.\")\n",
    "# --- CLOSE THE CLIENT BEFORE ZIPPING ---\n",
    "del chroma_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763491d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: chromadb_email_db/ (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/ (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/data_level0.bin (deflated 100%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/length.bin (deflated 77%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/link_lists.bin (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/header.bin (deflated 61%)\n",
      "  adding: chromadb_email_db/chroma.sqlite3 (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r chromadb_email_db.zip chromadb_email_db/  #jupyter run\n",
    "\n",
    "# Save the collection to disk(.py file run)\n",
    "#import subprocess\n",
    "#subprocess.run([\"zip\", \"-r\", \"chroma_email_db.zip\", \"chroma_email_db/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899f880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d905a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaDB loaded from GitHub.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "# ðŸ”— GitHub raw link to your zipped ChromaDB\n",
    "chroma_db_zip_url = \"https://raw.githubusercontent.com/kathisnehith/Linkedin-Jobs-posts-Scraper/main/data/chroma_email_db.zip\"\n",
    "zip_path = \"/tmp/chroma_email_db.zip\"\n",
    "extract_path = \"/tmp/chroma_email_db\"\n",
    "\n",
    "# ðŸ”½ Download and unzip\n",
    "response = requests.get(chroma_db_zip_url)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"âœ… ChromaDB loaded from GitHub.\")\n",
    "else:\n",
    "    raise Exception(f\"âŒ Failed to download zip: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c62041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_ZIP_LOCAL_PATH = r\"/Users/kathisnehith/Desktop/Gethire-ai/chromadb_email_db.zip\"      # path to your zipped chroma db\n",
    "EXTRACT_PATH = r\"/tmp/chromadb_email_db\"                # extract here\n",
    "COLLECTION_NAME = \"git_email_rag\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8151c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted ChromaDB to /tmp/chromadb_email_db\n",
      "ðŸ“¦ Available collections: []\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(CHROMA_ZIP_LOCAL_PATH, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(EXTRACT_PATH)\n",
    "print(f\"âœ… Extracted ChromaDB to {EXTRACT_PATH}\")\n",
    "chroma_client = chromadb.PersistentClient(path=EXTRACT_PATH)\n",
    "available_collections = chroma_client.list_collections()\n",
    "print(\"ðŸ“¦ Available collections:\", [col.name for col in available_collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "362c75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chromadb_email_db', 'chroma.sqlite3']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(EXTRACT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "print(\"ðŸ“Š Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33169940",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bafb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top Matches:\n",
      "\n",
      "ðŸŽï¸Result 1:\n",
      "Token count: 575\n",
      "Guided  Email  template  for  corporate.  \n",
      " \n",
      " \n",
      "Introduction  \n",
      "Networking  in  the  corporate  world  can  feel  daunting,  especially  when  reaching  out  about  job  \n",
      "opportunities.\n",
      " \n",
      "This\n",
      " \n",
      "email\n",
      " \n",
      "template\n",
      " \n",
      "is\n",
      " \n",
      "designed\n",
      " \n",
      "to\n",
      " \n",
      "help\n",
      " \n",
      "you\n",
      " \n",
      "craft\n",
      " \n",
      "professional,\n",
      " \n",
      "warm,\n",
      " \n",
      "and\n",
      " \n",
      "concise\n",
      " \n",
      "messages\n",
      " \n",
      "that\n",
      " \n",
      "subtly\n",
      " \n",
      "position\n",
      " \n",
      "you\n",
      " \n",
      "for\n",
      " \n",
      "an\n",
      " \n",
      "interview.\n",
      " \n",
      "Whether\n",
      " \n",
      "youâ€™re\n",
      " \n",
      "contacting\n",
      " \n",
      "a\n",
      " \n",
      "recruiter,\n",
      " \n",
      "hiring\n",
      " \n",
      "manager,\n",
      " \n",
      "senior\n",
      " \n",
      "employee,\n",
      " \n",
      "or\n",
      " \n",
      "peer,\n",
      " \n",
      "this\n",
      " \n",
      "flexible\n",
      " \n",
      "template\n",
      " \n",
      "adapts\n",
      " \n",
      "to\n",
      " \n",
      "their\n",
      " \n",
      "role\n",
      " \n",
      "and\n",
      " \n",
      "your\n",
      " \n",
      "goal.\n",
      " \n",
      "Below,\n",
      " \n",
      "youâ€™ll\n",
      " \n",
      "find\n",
      " \n",
      "the\n",
      " \n",
      "base\n",
      " \n",
      "template,\n",
      " \n",
      "step-by-step\n",
      " \n",
      "guidance\n",
      " \n",
      "on\n",
      " \n",
      "how\n",
      " \n",
      "to\n",
      " \n",
      "use\n",
      " \n",
      "it,\n",
      " \n",
      "and\n",
      " \n",
      "real-world\n",
      " \n",
      "examples\n",
      " \n",
      "to\n",
      " \n",
      "get\n",
      " \n",
      "you\n",
      " \n",
      "started.\n",
      " \n",
      "The  focus?  Highlight  your  fit  for  a  role  in  a  natural  way,  respect  the  recipientâ€™s  authority,  and  \n",
      "nudge\n",
      " \n",
      "toward\n",
      " \n",
      "a\n",
      " \n",
      "conversationâ€”all\n",
      " \n",
      "while\n",
      " \n",
      "keeping\n",
      " \n",
      "it\n",
      " \n",
      "human\n",
      " \n",
      "and\n",
      " \n",
      "approachable.\n",
      " \n",
      "How  to  Use  This  Template   \n",
      "This  template  is  your  starting  pointâ€”itâ€™s  flexible  enough  to  work  for  any  networking  reach-out.  \n",
      "Hereâ€™s\n",
      " \n",
      "how\n",
      " \n",
      "to\n",
      " \n",
      "customize\n",
      " \n",
      "it\n",
      " \n",
      "for\n",
      " \n",
      "your\n",
      " \n",
      "needs:\n",
      " \n",
      "Step  1:  Pick  Your  Recipient  and  Purpose  \n",
      "â—  Who  are  you  emailing?  A  recruiter,  hiring  manager,  senior  employee,  or  peer?  Each  \n",
      "has\n",
      " \n",
      "a\n",
      " \n",
      "different\n",
      " \n",
      "role\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "hiring\n",
      " \n",
      "process.\n",
      " â—  Whatâ€™s  your  goal?  Usually,  itâ€™s  to  subtly  push  for  an  interview,  but  you  might  also  want  \n",
      "advice\n",
      " \n",
      "or\n",
      " \n",
      "a\n",
      " \n",
      "referral.\n",
      " \n",
      "Step  2:  Fill  in  the  Placeholders  \n",
      "â—  Subject :  Keep  it  short  and  specific  (e.g.,  â€œExploring  the  Data  Analyst  Roleâ€).  â—  Opener :  Mention  why  youâ€™re  contacting  them  (e.g.,  a  job  post,  their  team,  or  their  \n",
      "expertise).\n",
      " â—  Subtle  Pitch :  â—‹  Key  Experience :  A  broad  skill  or  area  youâ€™re  strong  in  (e.g.,  â€œanalyze  dataâ€).\n",
      "-->> Metadata: {'section': 'email_guide'}\n",
      "\n",
      "ðŸŽï¸Result 2:\n",
      "Token count: 497\n",
      "Senior  Employee  â€“  Influencer  \n",
      "â—  Goal :  Gain  support  or  a  referral  by  showing  fit.  â—  Context :  Senior  employees  have  insights  and  influence  but  arenâ€™t  direct  \n",
      "decision-makers.\n",
      " \n",
      "The\n",
      " \n",
      "goal\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "gain\n",
      " \n",
      "their\n",
      " \n",
      "support\n",
      " \n",
      "or\n",
      " \n",
      "a\n",
      " \n",
      "referral.\n",
      " â—  Tone :  Respectful,  curious,  and  humbleâ€”acknowledging  their  expertise  while  subtly  \n",
      "showcasing\n",
      " \n",
      "fit.\n",
      " â—  Customizations :  â—‹  â€¢  Subject:  â€œYour  Take  on  [Company]â€™s  Data  Work?â€  â—‹  â€¢  Opener:  Connects  via  their  role  or  company.  â—‹  â€¢  Pitch:  Light  touch,  framing  your  work  as  relevant.  â—‹  â€¢  Request:  Seeks  guidance  to  build  a  bridge.  â—  Why  It  Works :  Flatters  their  expertise,  keeps  it  light.  \n",
      "Example :  \n",
      "Subject:  Your  Take  on  [Company]â€™s  Data  Work?  \n",
      "Hi  Emily,  \n",
      "I  hope  this  finds  you  well!  Iâ€™m  reaching  out  about  [Company]â€™s  work  in  data  \n",
      "analytics,\n",
      " \n",
      "which\n",
      " \n",
      "caught\n",
      " \n",
      "my\n",
      " \n",
      "eye\n",
      " \n",
      "as\n",
      " \n",
      "someone\n",
      " \n",
      "deeply\n",
      " \n",
      "interested\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "field.\n",
      " \n",
      "Iâ€™ve  had  the  chance  to  dig  into  complex  datasets,  like  optimizing  a  dashboard  with  \n",
      "Tableau\n",
      " \n",
      "that\n",
      " \n",
      "sped\n",
      " \n",
      "up\n",
      " \n",
      "decision-making.\n",
      " \n",
      "It\n",
      " \n",
      "feels\n",
      " \n",
      "like\n",
      " \n",
      "a\n",
      " \n",
      "natural\n",
      " \n",
      "fit\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "kind\n",
      " \n",
      "of\n",
      " \n",
      "impact\n",
      " \n",
      "your\n",
      " \n",
      "team\n",
      " \n",
      "might\n",
      " \n",
      "be\n",
      " \n",
      "driving,\n",
      " \n",
      "and\n",
      " \n",
      "Iâ€™d\n",
      " \n",
      "love\n",
      " \n",
      "to\n",
      " \n",
      "explore\n",
      " \n",
      "how\n",
      " \n",
      "I\n",
      " \n",
      "could\n",
      " \n",
      "contribute.\n",
      " \n",
      "Any  insights  on  what  makes  someone  stand  out  at  [Company]?  Iâ€™m  eager  to  learn  \n",
      "more.\n",
      " \n",
      "Thanks\n",
      " \n",
      "so\n",
      " \n",
      "much\n",
      " \n",
      "for\n",
      " \n",
      "your\n",
      " \n",
      "timeâ€”it\n",
      " \n",
      "means\n",
      " \n",
      "a\n",
      " \n",
      "lot!\n",
      " \n",
      "Best  regards,  \n",
      "[Your  Name]  \n",
      "[LinkedIn  Profile]\n",
      "-->> Metadata: {'section': 'senioremployee_template'}\n",
      "-->>ðŸ¤– Total token count: 1072\n",
      "ðŸ¤–Total tokens in combined context_text: 1073\n",
      "vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\n",
      "-------------------------------\n",
      "Subject: Your Insights on Data Engineering at [Company]  \n",
      "\n",
      "Hi [Recipient's Name],  \n",
      "\n",
      "I hope this email finds you well! Iâ€™m reaching out because I admire [Company]â€™s innovative work in data engineering and its commitment to solving complex problems at scale. As someone whoâ€™s deeply passionate about leveraging data to drive impactful decisions, Iâ€™d love to learn more about the exciting projects your team is working on.  \n",
      "\n",
      "Iâ€™ve been fortunate to work on projects such as [briefly mention a key project or achievement, e.g., â€œbuilding a scalable ETL pipeline that reduced data processing time by 30%â€]. I believe this aligns well with the kind of challenges your team might be tackling, and Iâ€™m eager to explore how my background could contribute to [Company]â€™s success.  \n",
      "\n",
      "If you have a moment, Iâ€™d truly appreciate any advice or insights you could share about what makes someone thrive on your team or at [Company]. Your perspective would mean a lot as I continue to grow in my career as a Senior Data Engineer.  \n",
      "\n",
      "Thank you so much for your time and guidance!  \n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "[Your LinkedIn Profile]  \n",
      "[Your Contact Information]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######\n",
    "## Retrieving a document from the collection\n",
    "#######\n",
    "\n",
    "user_purpose_prompt = \"Can you write a professional email connecting to network?\"\n",
    "persona = \"senior data engineer\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\"\n",
    "query_response = client.embeddings.create(\n",
    "    input=[retrival_query],\n",
    "    model=embedding_model\n",
    ")\n",
    "query_vector = query_response.data[0].embedding\n",
    "#print(f\"Query vector (first 5 dims): {query_vector[:5]}\")\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],\n",
    "    n_results=2   # no of results to return\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Top Matches:\")\n",
    "total_tokens = 0\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    token_count = num_tokens_from_string(doc, \"o200k_base\")\n",
    "    print(f\"\\nðŸŽï¸Result {i+1}:\")\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(doc)\n",
    "    print(\"-->> Metadata:\", results[\"metadatas\"][0][i])\n",
    "    total_tokens += token_count\n",
    "\n",
    "print(f\"-->>ðŸ¤– Total token count: {total_tokens}\")\n",
    "\n",
    "# Combine all retrieved docs into one context_text string (outside the loop)\n",
    "context_text = \"\\n\\n---\\n\\n\".join(results[\"documents\"][0])\n",
    "#print(\"\\nCombined context_text:\\n\", context_text)\n",
    "context_text_token=num_tokens_from_string(context_text, \"o200k_base\")\n",
    "print(f\"ðŸ¤–Total tokens in combined context_text: {context_text_token}\")\n",
    "\n",
    "\n",
    "###\n",
    "# Final prompt construction\n",
    "# Openai LLM call\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "final_prompt = f\"\"\" \n",
    "{retrival_query}\n",
    "==== CONTEXT START ====\n",
    "{context_text}\n",
    "==== CONTEXT END ====\n",
    "\"\"\"\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=model_name,  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in writing emails using context data, along with the user provided query.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "final_email_response=chat_response.choices[0].message.content\n",
    "print(\"vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\")\n",
    "print(\"-------------------------------\")\n",
    "print(final_email_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d21ea",
   "metadata": {},
   "source": [
    "## try-github-zip-embeed-extract-retrive-RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faba2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import chromadb\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785a7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaDB loaded from GitHub.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ðŸ”— GitHub raw link to your zipped ChromaDB\n",
    "chroma_db_zip_url = \"https://raw.githubusercontent.com/kathisnehith/Linkedin-Jobs-posts-Scraper/main/data/chroma_email_db.zip\"\n",
    "zip_path = \"/tmp/chroma_email_db.zip\"\n",
    "extract_path = \"/tmp/chroma_email_db\"\n",
    "\n",
    "# ðŸ”½ Download and unzip\n",
    "response = requests.get(chroma_db_zip_url)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"âœ… ChromaDB loaded from GitHub.\")\n",
    "else:\n",
    "    raise Exception(f\"âŒ Failed to download zip: {response.status_code}\")\n",
    "\n",
    "# âœ… Load collection from extracted path\n",
    "chroma_client = chromadb.PersistentClient(path=extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f96d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_collection(name=\"test_git_email_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df59103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "[Collection(name=test_git_email_rag)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Available collections:\")\n",
    "print(chroma_client.list_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d32ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f55a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528ebf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embed user query\n",
    "user_purpose_prompt = \"Can you write a professional email connecting to network?\"\n",
    "persona = \"senior data engineer\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\"\n",
    "query_response = client.embeddings.create(\n",
    "    input=[retrival_query],\n",
    "    model=embedding_model\n",
    ")\n",
    "query_vector = query_response.data[0].embedding\n",
    "\n",
    "# --- Search Chroma collection\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],\n",
    "    n_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5056ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [[]], 'embeddings': None, 'documents': [[]], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[]], 'distances': [[]]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c883c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1169c",
   "metadata": {},
   "source": [
    "## lanchain-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231d9f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/hiring_manager_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'hiring_manager_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/email_guide.pdf\n",
      "Number of chunks:---- 2\n",
      " metadata:---- 2 [{'section': 'email_guide'}, {'section': 'email_guide'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/recruiter_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'recruiter_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/senioremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'senioremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/peeremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'peeremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/base_email_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'base_email_template'}]\n",
      "Total chunks: 7\n",
      "First chunk metadata: {'section': 'hiring_manager_template'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import langchain\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "pdf_folder_path = r\"/Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide\"\n",
    "chunk_size_len = 2000\n",
    "chunk_overlap_len = 50\n",
    "email_text_chunks = []\n",
    "email_metadata = []\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        rag_file_path =os.path.join(pdf_folder_path, filename)\n",
    "        print(f\"Processing file: {rag_file_path}\")\n",
    "        loader = PyPDFLoader(file_path=rag_file_path)\n",
    "        document = loader.load()\n",
    "        text_splitter_char = CharacterTextSplitter(chunk_size=chunk_size_len, chunk_overlap=chunk_overlap_len, separator=\"\\n\")\n",
    "        split_documents = text_splitter_char.split_documents(document)\n",
    "        print(f\"Number of chunks:---- {len(split_documents)}\") \n",
    "        doc_tag = filename.split('.')[0]\n",
    "        metadatas = [{\"section\": doc_tag} for _ in split_documents]     \n",
    "        print(f\" metadata:---- {len(metadatas)} {metadatas}\")\n",
    "        email_text_chunks.extend(split_documents)\n",
    "        email_metadata.extend(metadatas) \n",
    "\n",
    "print(f\"Total chunks: {len(email_text_chunks)}\")\n",
    "print(f\"First chunk metadata: {email_metadata[0]}\")\n",
    "\n",
    "### tokenization function counting of texts\n",
    "def num_tokens_from_string(input_text: str, encoding_name: str) -> str:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(input_text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding function using OPENAI-EMBEDDING-3-LARGE\n",
    "\n",
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "embedding_model = \"text-embedding-3-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b760c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define persist directory path\n",
    "persist_dir = \"./chroma_persist_email_rag\"\n",
    "\n",
    "# Convert text chunks to LangChain Document objects\n",
    "langchain_docs = [\n",
    "    Document(page_content=chunk.page_content, metadata=email_metadata[i])\n",
    "    for i, chunk in enumerate(email_text_chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ef3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Embedding and creating persistent vectorstore...\n"
     ]
    }
   ],
   "source": [
    "# Check if vectorstore already exists, else create and persist\n",
    "if not os.path.exists(persist_dir):\n",
    "    print(\"ðŸ”„ Embedding and creating persistent vectorstore...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=langchain_docs,\n",
    "        embedding=OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=endpoint,\n",
    "            openai_api_key=token\n",
    "        ),\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=\"email_rag\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… Loading vectorstore from disk...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding=OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=endpoint,\n",
    "            openai_api_key=token\n",
    "        ),\n",
    "        collection_name=\"email_rag\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4db4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_purpose_prompt = \"Can you write a professional email for enquire on job openings?\"\n",
    "persona = \"  staffing firm representative\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6309e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top Matches with Scores:\n",
      "\n",
      "ðŸŽï¸ Result 1 (Score: 0.9333)\n",
      "Token count: 575\n",
      "-->>â€¼ï¸ Metadata: {'section': 'email_guide'}\n",
      "\n",
      "ðŸŽï¸ Result 2 (Score: 0.9941)\n",
      "Token count: 514\n",
      "-->>â€¼ï¸ Metadata: {'section': 'hiring_manager_template'}\n",
      "-->> ðŸ¤– Total token count: 1089\n",
      "ðŸ¤– Total tokens in combined context_text: 1090\n"
     ]
    }
   ],
   "source": [
    "# Run similarity search with score\n",
    "results_with_scores = vectorstore.similarity_search_with_score(retrival_query, k=2)\n",
    "\n",
    "print(\"\\nðŸ” Top Matches with Scores:\")\n",
    "total_tokens = 0\n",
    "retrieved_docs = []\n",
    "for i, (doc, score) in enumerate(results_with_scores):\n",
    "    token_count = num_tokens_from_string(doc.page_content, \"o200k_base\")\n",
    "    print(f\"\\nðŸŽï¸ Result {i+1} (Score: {score:.4f})\")\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    #print(doc.page_content)\n",
    "    print(\"-->>â€¼ï¸ Metadata:\", doc.metadata)\n",
    "    total_tokens += token_count\n",
    "    retrieved_docs.append(doc.page_content)\n",
    "\n",
    "print(f\"-->> ðŸ¤– Total token count: {total_tokens}\")\n",
    "\n",
    "# Combine context\n",
    "context_text = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
    "context_text_token = num_tokens_from_string(context_text, \"o200k_base\")\n",
    "print(f\"ðŸ¤– Total tokens in combined context_text: {context_text_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d0c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Total tokens in combined context_text: 1120\n",
      "vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\n",
      "-------------------------------\n",
      "Subject: Inquiry About Job Opportunities at [Staffing Firm Name]  \n",
      "\n",
      "Dear [Recipient's Name],  \n",
      "\n",
      "I hope this email finds you well! My name is [Your Name], and Iâ€™m reaching out to inquire about any current or upcoming job openings your team at [Staffing Firm Name] may have available.  \n",
      "\n",
      "With a background in [Your Industry/Field, e.g., talent acquisition, project management, etc.] and a passion for connecting the right individuals with opportunities that align with their skills, Iâ€™ve been following the impactful work your firm does in the staffing industry.  \n",
      "\n",
      "In my [X years] of experience, Iâ€™ve had the privilege of [Key Achievement/Experience, e.g., managing end-to-end recruitment for high-growth teams or building strategic partnerships that enhanced talent pipelines]. I believe my expertise in [Specific Skill or Area, e.g., sourcing top-tier talent, streamlining hiring processes, etc.] could align well with your teamâ€™s goals.  \n",
      "\n",
      "Iâ€™d love to learn more about how I might contribute to the success of your firm. Could we schedule a time to discuss any potential openings or how my background might fit with your team?  \n",
      "\n",
      "Thank you so much for your time and consideration. I look forward to hearing from you!  \n",
      "\n",
      "Best regards,  \n",
      "[Your Full Name]  \n",
      "[Your LinkedIn Profile]  \n",
      "[Your Contact Information]  \n",
      "ðŸ¤– Total tokens in combined context_text: 276\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Final prompt construction\n",
    "# Openai LLM call\n",
    "###\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")\n",
    "\n",
    "final_prompt = f\"\"\" \n",
    "{retrival_query}\n",
    "==== CONTEXT START ====\n",
    "{context_text}\n",
    "==== CONTEXT END ====\n",
    "\"\"\"\n",
    "total_tokens = num_tokens_from_string(final_prompt, \"o200k_base\")\n",
    "print(f\"ðŸ¤– Total tokens in combined context_text: {total_tokens}\")\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=model_name,  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in writing emails using context data, along with the user provided query.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "final_email_response=chat_response.choices[0].message.content\n",
    "print(\"vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\")\n",
    "print(\"-------------------------------\")\n",
    "print(final_email_response)\n",
    "email_output_tokens=num_tokens_from_string(final_email_response, \"o200k_base\")\n",
    "print(f\"ðŸ¤– Total tokens in combined context_text: {email_output_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
