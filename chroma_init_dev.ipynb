{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d4b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/hiring_manager_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'hiring_manager_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/email_guide.pdf\n",
      "Number of chunks:---- 2\n",
      " metadata:---- 2 [{'section': 'email_guide'}, {'section': 'email_guide'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/recruiter_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'recruiter_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/senioremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'senioremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/peeremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'peeremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/base_email_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'base_email_template'}]\n",
      "Total chunks: 7\n",
      "First chunk metadata: {'section': 'hiring_manager_template'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import langchain\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "pdf_folder_path = r\"/Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide\"\n",
    "chunk_size_len = 2000\n",
    "chunk_overlap_len = 50\n",
    "email_text_chunks = []\n",
    "email_metadata = []\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        rag_file_path =os.path.join(pdf_folder_path, filename)\n",
    "        print(f\"Processing file: {rag_file_path}\")\n",
    "        loader = PyPDFLoader(file_path=rag_file_path)\n",
    "        document = loader.load()\n",
    "        text_splitter_char = CharacterTextSplitter(chunk_size=chunk_size_len, chunk_overlap=chunk_overlap_len, separator=\"\\n\")\n",
    "        split_documents = text_splitter_char.split_documents(document)\n",
    "        print(f\"Number of chunks:---- {len(split_documents)}\") \n",
    "        doc_tag = filename.split('.')[0]\n",
    "        metadatas = [{\"section\": doc_tag} for _ in split_documents]     \n",
    "        print(f\" metadata:---- {len(metadatas)} {metadatas}\")\n",
    "        email_text_chunks.extend(split_documents)\n",
    "        email_metadata.extend(metadatas) \n",
    "\n",
    "print(f\"Total chunks: {len(email_text_chunks)}\")\n",
    "print(f\"First chunk metadata: {email_metadata[0]}\")\n",
    "\n",
    "### tokenization function counting of texts\n",
    "def num_tokens_from_string(input_text: str, encoding_name: str) -> str:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(input_text))\n",
    "    return num_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef718456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 metadata: {'section': 'hiring_manager_template'}\n",
      "Embedding Vector (first 5 dims): [0.000710678577888757, -0.006397498305886984, -0.004142611753195524, -0.035692475736141205, -0.015235316939651966]\n",
      "Chunk 1 metadata: {'section': 'email_guide'}\n",
      "Embedding Vector (first 5 dims): [-0.017011938616633415, -0.031198788434267044, -0.007742120418697596, -0.04854850843548775, -0.012551678344607353]\n",
      "Chunk 2 metadata: {'section': 'email_guide'}\n",
      "Embedding Vector (first 5 dims): [-0.007910573855042458, -0.040177181363105774, -0.0038782358169555664, -0.01838161237537861, -0.01209108717739582]\n",
      "Chunk 3 metadata: {'section': 'recruiter_template'}\n",
      "Embedding Vector (first 5 dims): [-0.019944479689002037, -0.03546753153204918, -0.012463296763598919, -0.02058526501059532, -0.03879962116479874]\n",
      "Chunk 4 metadata: {'section': 'senioremployee_template'}\n",
      "Embedding Vector (first 5 dims): [-0.018149359151721, 0.008191158063709736, -0.01531713455915451, -0.015003359876573086, -0.015919910743832588]\n",
      "Chunk 5 metadata: {'section': 'peeremployee_template'}\n",
      "Embedding Vector (first 5 dims): [-0.03984778746962547, 0.004485694691538811, -0.018184378743171692, -0.02813827246427536, -0.0160583034157753]\n",
      "Chunk 6 metadata: {'section': 'base_email_template'}\n",
      "Embedding Vector (first 5 dims): [0.0030544132459908724, -0.026033055037260056, -0.006182117387652397, -0.032746534794569016, -0.017941700294613838]\n"
     ]
    }
   ],
   "source": [
    "## embedding function using OPENAI-EMBEDDING-3-LARGE\n",
    "\n",
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=[chunk.page_content for chunk in email_text_chunks],\n",
    "    model=embedding_model,\n",
    ")\n",
    "#print(response)\n",
    "embeddings = [item.embedding for item in response.data]\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Display results\n",
    "for i, item in enumerate(response.data):\n",
    "    print(f\"Chunk {i} metadata: {email_metadata[i]}\")\n",
    "    print(f\"Embedding Vector (first 5 dims): {item.embedding[:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29579bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"./chromadb_email_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"git_email_rag\")\n",
    "collection.add(\n",
    "    documents=[chunk.page_content for chunk in email_text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    metadatas=email_metadata,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(email_text_chunks))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de451e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chroma collection populated with 7 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "doc_count = collection.count()\n",
    "print(f\"‚úÖ Chroma collection populated with {doc_count} documents\")\n",
    "\n",
    "# üîí Prevent accidental zipping of empty DB\n",
    "if doc_count == 0:\n",
    "    raise Exception(\"‚ùå ERROR: No documents added to ChromaDB! Skipping zip to avoid saving empty DB.\")\n",
    "# --- CLOSE THE CLIENT BEFORE ZIPPING ---\n",
    "del chroma_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763491d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: chromadb_email_db/ (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/ (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/data_level0.bin (deflated 100%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/length.bin (deflated 77%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/link_lists.bin (stored 0%)\n",
      "  adding: chromadb_email_db/bc595fdf-315d-4360-9abe-3a795c428bb6/header.bin (deflated 61%)\n",
      "  adding: chromadb_email_db/chroma.sqlite3 (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r chromadb_email_db.zip chromadb_email_db/  #jupyter run\n",
    "\n",
    "# Save the collection to disk(.py file run)\n",
    "#import subprocess\n",
    "#subprocess.run([\"zip\", \"-r\", \"chroma_email_db.zip\", \"chroma_email_db/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899f880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d905a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB loaded from GitHub.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "# üîó GitHub raw link to your zipped ChromaDB\n",
    "chroma_db_zip_url = \"https://raw.githubusercontent.com/kathisnehith/Linkedin-Jobs-posts-Scraper/main/data/chroma_email_db.zip\"\n",
    "zip_path = \"/tmp/chroma_email_db.zip\"\n",
    "extract_path = \"/tmp/chroma_email_db\"\n",
    "\n",
    "# üîΩ Download and unzip\n",
    "response = requests.get(chroma_db_zip_url)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"‚úÖ ChromaDB loaded from GitHub.\")\n",
    "else:\n",
    "    raise Exception(f\"‚ùå Failed to download zip: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c62041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_ZIP_LOCAL_PATH = r\"/Users/kathisnehith/Desktop/Gethire-ai/chromadb_email_db.zip\"      # path to your zipped chroma db\n",
    "EXTRACT_PATH = r\"/tmp/chromadb_email_db\"                # extract here\n",
    "COLLECTION_NAME = \"git_email_rag\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8151c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted ChromaDB to /tmp/chromadb_email_db\n",
      "üì¶ Available collections: []\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(CHROMA_ZIP_LOCAL_PATH, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(EXTRACT_PATH)\n",
    "print(f\"‚úÖ Extracted ChromaDB to {EXTRACT_PATH}\")\n",
    "chroma_client = chromadb.PersistentClient(path=EXTRACT_PATH)\n",
    "available_collections = chroma_client.list_collections()\n",
    "print(\"üì¶ Available collections:\", [col.name for col in available_collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "362c75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chromadb_email_db', 'chroma.sqlite3']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(EXTRACT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "print(\"üìä Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33169940",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bafb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top Matches:\n",
      "\n",
      "üèéÔ∏èResult 1:\n",
      "Token count: 575\n",
      "Guided  Email  template  for  corporate.  \n",
      " \n",
      " \n",
      "Introduction  \n",
      "Networking  in  the  corporate  world  can  feel  daunting,  especially  when  reaching  out  about  job  \n",
      "opportunities.\n",
      " \n",
      "This\n",
      " \n",
      "email\n",
      " \n",
      "template\n",
      " \n",
      "is\n",
      " \n",
      "designed\n",
      " \n",
      "to\n",
      " \n",
      "help\n",
      " \n",
      "you\n",
      " \n",
      "craft\n",
      " \n",
      "professional,\n",
      " \n",
      "warm,\n",
      " \n",
      "and\n",
      " \n",
      "concise\n",
      " \n",
      "messages\n",
      " \n",
      "that\n",
      " \n",
      "subtly\n",
      " \n",
      "position\n",
      " \n",
      "you\n",
      " \n",
      "for\n",
      " \n",
      "an\n",
      " \n",
      "interview.\n",
      " \n",
      "Whether\n",
      " \n",
      "you‚Äôre\n",
      " \n",
      "contacting\n",
      " \n",
      "a\n",
      " \n",
      "recruiter,\n",
      " \n",
      "hiring\n",
      " \n",
      "manager,\n",
      " \n",
      "senior\n",
      " \n",
      "employee,\n",
      " \n",
      "or\n",
      " \n",
      "peer,\n",
      " \n",
      "this\n",
      " \n",
      "flexible\n",
      " \n",
      "template\n",
      " \n",
      "adapts\n",
      " \n",
      "to\n",
      " \n",
      "their\n",
      " \n",
      "role\n",
      " \n",
      "and\n",
      " \n",
      "your\n",
      " \n",
      "goal.\n",
      " \n",
      "Below,\n",
      " \n",
      "you‚Äôll\n",
      " \n",
      "find\n",
      " \n",
      "the\n",
      " \n",
      "base\n",
      " \n",
      "template,\n",
      " \n",
      "step-by-step\n",
      " \n",
      "guidance\n",
      " \n",
      "on\n",
      " \n",
      "how\n",
      " \n",
      "to\n",
      " \n",
      "use\n",
      " \n",
      "it,\n",
      " \n",
      "and\n",
      " \n",
      "real-world\n",
      " \n",
      "examples\n",
      " \n",
      "to\n",
      " \n",
      "get\n",
      " \n",
      "you\n",
      " \n",
      "started.\n",
      " \n",
      "The  focus?  Highlight  your  fit  for  a  role  in  a  natural  way,  respect  the  recipient‚Äôs  authority,  and  \n",
      "nudge\n",
      " \n",
      "toward\n",
      " \n",
      "a\n",
      " \n",
      "conversation‚Äîall\n",
      " \n",
      "while\n",
      " \n",
      "keeping\n",
      " \n",
      "it\n",
      " \n",
      "human\n",
      " \n",
      "and\n",
      " \n",
      "approachable.\n",
      " \n",
      "How  to  Use  This  Template   \n",
      "This  template  is  your  starting  point‚Äîit‚Äôs  flexible  enough  to  work  for  any  networking  reach-out.  \n",
      "Here‚Äôs\n",
      " \n",
      "how\n",
      " \n",
      "to\n",
      " \n",
      "customize\n",
      " \n",
      "it\n",
      " \n",
      "for\n",
      " \n",
      "your\n",
      " \n",
      "needs:\n",
      " \n",
      "Step  1:  Pick  Your  Recipient  and  Purpose  \n",
      "‚óè  Who  are  you  emailing?  A  recruiter,  hiring  manager,  senior  employee,  or  peer?  Each  \n",
      "has\n",
      " \n",
      "a\n",
      " \n",
      "different\n",
      " \n",
      "role\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "hiring\n",
      " \n",
      "process.\n",
      " ‚óè  What‚Äôs  your  goal?  Usually,  it‚Äôs  to  subtly  push  for  an  interview,  but  you  might  also  want  \n",
      "advice\n",
      " \n",
      "or\n",
      " \n",
      "a\n",
      " \n",
      "referral.\n",
      " \n",
      "Step  2:  Fill  in  the  Placeholders  \n",
      "‚óè  Subject :  Keep  it  short  and  specific  (e.g.,  ‚ÄúExploring  the  Data  Analyst  Role‚Äù).  ‚óè  Opener :  Mention  why  you‚Äôre  contacting  them  (e.g.,  a  job  post,  their  team,  or  their  \n",
      "expertise).\n",
      " ‚óè  Subtle  Pitch :  ‚óã  Key  Experience :  A  broad  skill  or  area  you‚Äôre  strong  in  (e.g.,  ‚Äúanalyze  data‚Äù).\n",
      "-->> Metadata: {'section': 'email_guide'}\n",
      "\n",
      "üèéÔ∏èResult 2:\n",
      "Token count: 497\n",
      "Senior  Employee  ‚Äì  Influencer  \n",
      "‚óè  Goal :  Gain  support  or  a  referral  by  showing  fit.  ‚óè  Context :  Senior  employees  have  insights  and  influence  but  aren‚Äôt  direct  \n",
      "decision-makers.\n",
      " \n",
      "The\n",
      " \n",
      "goal\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "gain\n",
      " \n",
      "their\n",
      " \n",
      "support\n",
      " \n",
      "or\n",
      " \n",
      "a\n",
      " \n",
      "referral.\n",
      " ‚óè  Tone :  Respectful,  curious,  and  humble‚Äîacknowledging  their  expertise  while  subtly  \n",
      "showcasing\n",
      " \n",
      "fit.\n",
      " ‚óè  Customizations :  ‚óã  ‚Ä¢  Subject:  ‚ÄúYour  Take  on  [Company]‚Äôs  Data  Work?‚Äù  ‚óã  ‚Ä¢  Opener:  Connects  via  their  role  or  company.  ‚óã  ‚Ä¢  Pitch:  Light  touch,  framing  your  work  as  relevant.  ‚óã  ‚Ä¢  Request:  Seeks  guidance  to  build  a  bridge.  ‚óè  Why  It  Works :  Flatters  their  expertise,  keeps  it  light.  \n",
      "Example :  \n",
      "Subject:  Your  Take  on  [Company]‚Äôs  Data  Work?  \n",
      "Hi  Emily,  \n",
      "I  hope  this  finds  you  well!  I‚Äôm  reaching  out  about  [Company]‚Äôs  work  in  data  \n",
      "analytics,\n",
      " \n",
      "which\n",
      " \n",
      "caught\n",
      " \n",
      "my\n",
      " \n",
      "eye\n",
      " \n",
      "as\n",
      " \n",
      "someone\n",
      " \n",
      "deeply\n",
      " \n",
      "interested\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "field.\n",
      " \n",
      "I‚Äôve  had  the  chance  to  dig  into  complex  datasets,  like  optimizing  a  dashboard  with  \n",
      "Tableau\n",
      " \n",
      "that\n",
      " \n",
      "sped\n",
      " \n",
      "up\n",
      " \n",
      "decision-making.\n",
      " \n",
      "It\n",
      " \n",
      "feels\n",
      " \n",
      "like\n",
      " \n",
      "a\n",
      " \n",
      "natural\n",
      " \n",
      "fit\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "kind\n",
      " \n",
      "of\n",
      " \n",
      "impact\n",
      " \n",
      "your\n",
      " \n",
      "team\n",
      " \n",
      "might\n",
      " \n",
      "be\n",
      " \n",
      "driving,\n",
      " \n",
      "and\n",
      " \n",
      "I‚Äôd\n",
      " \n",
      "love\n",
      " \n",
      "to\n",
      " \n",
      "explore\n",
      " \n",
      "how\n",
      " \n",
      "I\n",
      " \n",
      "could\n",
      " \n",
      "contribute.\n",
      " \n",
      "Any  insights  on  what  makes  someone  stand  out  at  [Company]?  I‚Äôm  eager  to  learn  \n",
      "more.\n",
      " \n",
      "Thanks\n",
      " \n",
      "so\n",
      " \n",
      "much\n",
      " \n",
      "for\n",
      " \n",
      "your\n",
      " \n",
      "time‚Äîit\n",
      " \n",
      "means\n",
      " \n",
      "a\n",
      " \n",
      "lot!\n",
      " \n",
      "Best  regards,  \n",
      "[Your  Name]  \n",
      "[LinkedIn  Profile]\n",
      "-->> Metadata: {'section': 'senioremployee_template'}\n",
      "-->>ü§ñ Total token count: 1072\n",
      "ü§ñTotal tokens in combined context_text: 1073\n",
      "vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\n",
      "-------------------------------\n",
      "Subject: Your Insights on Data Engineering at [Company]  \n",
      "\n",
      "Hi [Recipient's Name],  \n",
      "\n",
      "I hope this email finds you well! I‚Äôm reaching out because I admire [Company]‚Äôs innovative work in data engineering and its commitment to solving complex problems at scale. As someone who‚Äôs deeply passionate about leveraging data to drive impactful decisions, I‚Äôd love to learn more about the exciting projects your team is working on.  \n",
      "\n",
      "I‚Äôve been fortunate to work on projects such as [briefly mention a key project or achievement, e.g., ‚Äúbuilding a scalable ETL pipeline that reduced data processing time by 30%‚Äù]. I believe this aligns well with the kind of challenges your team might be tackling, and I‚Äôm eager to explore how my background could contribute to [Company]‚Äôs success.  \n",
      "\n",
      "If you have a moment, I‚Äôd truly appreciate any advice or insights you could share about what makes someone thrive on your team or at [Company]. Your perspective would mean a lot as I continue to grow in my career as a Senior Data Engineer.  \n",
      "\n",
      "Thank you so much for your time and guidance!  \n",
      "\n",
      "Best regards,  \n",
      "[Your Name]  \n",
      "[Your LinkedIn Profile]  \n",
      "[Your Contact Information]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######\n",
    "## Retrieving a document from the collection\n",
    "#######\n",
    "\n",
    "user_purpose_prompt = \"Can you write a professional email connecting to network?\"\n",
    "persona = \"senior data engineer\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\"\n",
    "query_response = client.embeddings.create(\n",
    "    input=[retrival_query],\n",
    "    model=embedding_model\n",
    ")\n",
    "query_vector = query_response.data[0].embedding\n",
    "#print(f\"Query vector (first 5 dims): {query_vector[:5]}\")\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],\n",
    "    n_results=2   # no of results to return\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Top Matches:\")\n",
    "total_tokens = 0\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    token_count = num_tokens_from_string(doc, \"o200k_base\")\n",
    "    print(f\"\\nüèéÔ∏èResult {i+1}:\")\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(doc)\n",
    "    print(\"-->> Metadata:\", results[\"metadatas\"][0][i])\n",
    "    total_tokens += token_count\n",
    "\n",
    "print(f\"-->>ü§ñ Total token count: {total_tokens}\")\n",
    "\n",
    "# Combine all retrieved docs into one context_text string (outside the loop)\n",
    "context_text = \"\\n\\n---\\n\\n\".join(results[\"documents\"][0])\n",
    "#print(\"\\nCombined context_text:\\n\", context_text)\n",
    "context_text_token=num_tokens_from_string(context_text, \"o200k_base\")\n",
    "print(f\"ü§ñTotal tokens in combined context_text: {context_text_token}\")\n",
    "\n",
    "\n",
    "###\n",
    "# Final prompt construction\n",
    "# Openai LLM call\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "final_prompt = f\"\"\" \n",
    "{retrival_query}\n",
    "==== CONTEXT START ====\n",
    "{context_text}\n",
    "==== CONTEXT END ====\n",
    "\"\"\"\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=model_name,  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in writing emails using context data, along with the user provided query.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "final_email_response=chat_response.choices[0].message.content\n",
    "print(\"vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\")\n",
    "print(\"-------------------------------\")\n",
    "print(final_email_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d21ea",
   "metadata": {},
   "source": [
    "## try-github-zip-embeed-extract-retrive-RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faba2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import chromadb\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785a7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB loaded from GitHub.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# üîó GitHub raw link to your zipped ChromaDB\n",
    "chroma_db_zip_url = \"https://raw.githubusercontent.com/kathisnehith/Linkedin-Jobs-posts-Scraper/main/data/chroma_email_db.zip\"\n",
    "zip_path = \"/tmp/chroma_email_db.zip\"\n",
    "extract_path = \"/tmp/chroma_email_db\"\n",
    "\n",
    "# üîΩ Download and unzip\n",
    "response = requests.get(chroma_db_zip_url)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"‚úÖ ChromaDB loaded from GitHub.\")\n",
    "else:\n",
    "    raise Exception(f\"‚ùå Failed to download zip: {response.status_code}\")\n",
    "\n",
    "# ‚úÖ Load collection from extracted path\n",
    "chroma_client = chromadb.PersistentClient(path=extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f96d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_collection(name=\"test_git_email_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df59103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "[Collection(name=test_git_email_rag)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Available collections:\")\n",
    "print(chroma_client.list_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d32ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f55a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528ebf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embed user query\n",
    "user_purpose_prompt = \"Can you write a professional email connecting to network?\"\n",
    "persona = \"senior data engineer\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\"\n",
    "query_response = client.embeddings.create(\n",
    "    input=[retrival_query],\n",
    "    model=embedding_model\n",
    ")\n",
    "query_vector = query_response.data[0].embedding\n",
    "\n",
    "# --- Search Chroma collection\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vector],\n",
    "    n_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5056ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [[]], 'embeddings': None, 'documents': [[]], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[]], 'distances': [[]]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c883c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents in collection:\", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1169c",
   "metadata": {},
   "source": [
    "## lanchain-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231d9f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/hiring_manager_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'hiring_manager_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/email_guide.pdf\n",
      "Number of chunks:---- 2\n",
      " metadata:---- 2 [{'section': 'email_guide'}, {'section': 'email_guide'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/recruiter_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'recruiter_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/senioremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'senioremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/peeremployee_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'peeremployee_template'}]\n",
      "Processing file: /Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide/base_email_template.pdf\n",
      "Number of chunks:---- 1\n",
      " metadata:---- 1 [{'section': 'base_email_template'}]\n",
      "Total chunks: 7\n",
      "First chunk metadata: {'section': 'hiring_manager_template'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import langchain\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "pdf_folder_path = r\"/Users/kathisnehith/Desktop/Gethire-ai/email_rag_guide\"\n",
    "chunk_size_len = 2000\n",
    "chunk_overlap_len = 50\n",
    "email_text_chunks = []\n",
    "email_metadata = []\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        rag_file_path =os.path.join(pdf_folder_path, filename)\n",
    "        print(f\"Processing file: {rag_file_path}\")\n",
    "        loader = PyPDFLoader(file_path=rag_file_path)\n",
    "        document = loader.load()\n",
    "        text_splitter_char = CharacterTextSplitter(chunk_size=chunk_size_len, chunk_overlap=chunk_overlap_len, separator=\"\\n\")\n",
    "        split_documents = text_splitter_char.split_documents(document)\n",
    "        print(f\"Number of chunks:---- {len(split_documents)}\") \n",
    "        doc_tag = filename.split('.')[0]\n",
    "        metadatas = [{\"section\": doc_tag} for _ in split_documents]     \n",
    "        print(f\" metadata:---- {len(metadatas)} {metadatas}\")\n",
    "        email_text_chunks.extend(split_documents)\n",
    "        email_metadata.extend(metadatas) \n",
    "\n",
    "print(f\"Total chunks: {len(email_text_chunks)}\")\n",
    "print(f\"First chunk metadata: {email_metadata[0]}\")\n",
    "\n",
    "### tokenization function counting of texts\n",
    "def num_tokens_from_string(input_text: str, encoding_name: str) -> str:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(input_text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding function using OPENAI-EMBEDDING-3-LARGE\n",
    "\n",
    "load_dotenv()\n",
    "token =os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name=\"gpt-4o-mini\"\n",
    "embedding_model = \"text-embedding-3-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b760c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define persist directory path\n",
    "persist_dir = \"./chroma_persist_email_rag\"\n",
    "\n",
    "# Convert text chunks to LangChain Document objects\n",
    "langchain_docs = [\n",
    "    Document(page_content=chunk.page_content, metadata=email_metadata[i])\n",
    "    for i, chunk in enumerate(email_text_chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ef3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Embedding and creating persistent vectorstore...\n"
     ]
    }
   ],
   "source": [
    "# Check if vectorstore already exists, else create and persist\n",
    "if not os.path.exists(persist_dir):\n",
    "    print(\"üîÑ Embedding and creating persistent vectorstore...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=langchain_docs,\n",
    "        embedding=OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=endpoint,\n",
    "            openai_api_key=token\n",
    "        ),\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=\"email_rag\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ Loading vectorstore from disk...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding=OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=endpoint,\n",
    "            openai_api_key=token\n",
    "        ),\n",
    "        collection_name=\"email_rag\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4db4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_purpose_prompt = \"Can you write a professional email for enquire on job openings?\"\n",
    "persona = \"  staffing firm representative\"\n",
    "\n",
    "retrival_query = f\"\"\"{user_purpose_prompt}  {persona} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6309e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top Matches with Scores:\n",
      "\n",
      "üèéÔ∏è Result 1 (Score: 0.9333)\n",
      "Token count: 575\n",
      "-->>‚ÄºÔ∏è Metadata: {'section': 'email_guide'}\n",
      "\n",
      "üèéÔ∏è Result 2 (Score: 0.9941)\n",
      "Token count: 514\n",
      "-->>‚ÄºÔ∏è Metadata: {'section': 'hiring_manager_template'}\n",
      "-->> ü§ñ Total token count: 1089\n",
      "ü§ñ Total tokens in combined context_text: 1090\n"
     ]
    }
   ],
   "source": [
    "# Run similarity search with score\n",
    "results_with_scores = vectorstore.similarity_search_with_score(retrival_query, k=2)\n",
    "\n",
    "print(\"\\nüîç Top Matches with Scores:\")\n",
    "total_tokens = 0\n",
    "retrieved_docs = []\n",
    "for i, (doc, score) in enumerate(results_with_scores):\n",
    "    token_count = num_tokens_from_string(doc.page_content, \"o200k_base\")\n",
    "    print(f\"\\nüèéÔ∏è Result {i+1} (Score: {score:.4f})\")\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    #print(doc.page_content)\n",
    "    print(\"-->>‚ÄºÔ∏è Metadata:\", doc.metadata)\n",
    "    total_tokens += token_count\n",
    "    retrieved_docs.append(doc.page_content)\n",
    "\n",
    "print(f\"-->> ü§ñ Total token count: {total_tokens}\")\n",
    "\n",
    "# Combine context\n",
    "context_text = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
    "context_text_token = num_tokens_from_string(context_text, \"o200k_base\")\n",
    "print(f\"ü§ñ Total tokens in combined context_text: {context_text_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d0c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Total tokens in combined context_text: 1120\n",
      "vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\n",
      "-------------------------------\n",
      "Subject: Inquiry About Job Opportunities at [Staffing Firm Name]  \n",
      "\n",
      "Dear [Recipient's Name],  \n",
      "\n",
      "I hope this email finds you well! My name is [Your Name], and I‚Äôm reaching out to inquire about any current or upcoming job openings your team at [Staffing Firm Name] may have available.  \n",
      "\n",
      "With a background in [Your Industry/Field, e.g., talent acquisition, project management, etc.] and a passion for connecting the right individuals with opportunities that align with their skills, I‚Äôve been following the impactful work your firm does in the staffing industry.  \n",
      "\n",
      "In my [X years] of experience, I‚Äôve had the privilege of [Key Achievement/Experience, e.g., managing end-to-end recruitment for high-growth teams or building strategic partnerships that enhanced talent pipelines]. I believe my expertise in [Specific Skill or Area, e.g., sourcing top-tier talent, streamlining hiring processes, etc.] could align well with your team‚Äôs goals.  \n",
      "\n",
      "I‚Äôd love to learn more about how I might contribute to the success of your firm. Could we schedule a time to discuss any potential openings or how my background might fit with your team?  \n",
      "\n",
      "Thank you so much for your time and consideration. I look forward to hearing from you!  \n",
      "\n",
      "Best regards,  \n",
      "[Your Full Name]  \n",
      "[Your LinkedIn Profile]  \n",
      "[Your Contact Information]  \n",
      "ü§ñ Total tokens in combined context_text: 276\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Final prompt construction\n",
    "# Openai LLM call\n",
    "###\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token\n",
    ")\n",
    "\n",
    "final_prompt = f\"\"\" \n",
    "{retrival_query}\n",
    "==== CONTEXT START ====\n",
    "{context_text}\n",
    "==== CONTEXT END ====\n",
    "\"\"\"\n",
    "total_tokens = num_tokens_from_string(final_prompt, \"o200k_base\")\n",
    "print(f\"ü§ñ Total tokens in combined context_text: {total_tokens}\")\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=model_name,  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in writing emails using context data, along with the user provided query.\"},\n",
    "        {\"role\": \"user\", \"content\": final_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "final_email_response=chat_response.choices[0].message.content\n",
    "print(\"vvvvvvvvvv  Generated Email Response  vvvvvvvvvvv\")\n",
    "print(\"-------------------------------\")\n",
    "print(final_email_response)\n",
    "email_output_tokens=num_tokens_from_string(final_email_response, \"o200k_base\")\n",
    "print(f\"ü§ñ Total tokens in combined context_text: {email_output_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
